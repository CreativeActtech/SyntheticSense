
<div align="center">
  <h1>SyntheticSense1.1 </h1>
  <img src="https://github.com/Jewelzufo/SyntheticSense1.1/blob/main/SyntheticSense.png?raw=true" height="500" width="500">
</div>

<div align="center">
  Date: 11-18-2025 | Version: 1.1 | License Apache 2.0
</div>

## About

SyntheticSense is a conceptual project that aims to provide an AI-powered solution for enhanced spatial awareness. This is for explorers looking to push the boundaries of Edge AI.

<div align="center">
  <h1>Conceptual Diagrams</h1>
</div>

## 1. Raspberry Pi 5 Configuration

<div align="center">
 <img src="https://github.com/CreativeActtech/SyntheticSense/blob/main/RPI-5.jpg?raw=true" height="600" width="600">
</div>

This diagram illustrates a wearable haptic interface that helps users detect obstacles through vibration. When the AI camera spots an object, the Raspberry Pi 5 activates motors on the side closest to the object—vibrating to alert the user.

This is the central configuration, which various configurations and features can be added in future iterations.

---

## 2. User Interface 

<div align="center">
 <img src="https://github.com/CreativeActtech/SyntheticSense/blob/main/ui_config2.jpg?raw=true" height="600" width="600">
</div>

This diagram presents a multi-directional haptic navigation interface designed to assist DeafBlind individuals with basic spatial awareness. When an object is detected, specific motors vibrate in directional patterns—forward, back, left, right, or multi-directional—alerting the user to the obstacle’s location. The tactile feedback helps guide navigation without relying on sight or sound.

---
